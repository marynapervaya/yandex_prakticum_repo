{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Цель: обучить модель классифицировать комментарии на позитивные и негативные на наборе данных с разметкой о токсичности правок.\n",
    "\n",
    "Ход исследования: Данные представлены в файле toxic_comments.csv.\n",
    "\n",
    "О качестве данных ничего неизвестно, поэтому перед проведением исследования понадобится обзор данных.\n",
    "\n",
    "Таким образом, исследование пройдет в 3 этапа:\n",
    "\n",
    "Открытие файлов с данными и их анализ;\n",
    "Обучение моделей;\n",
    "Общий вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing\n",
      "30%\n",
      "70%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "#загрузка библиотек\n",
    "print(\"importing\")\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import xgboost\n",
    "import numpy as np\n",
    "print(\"30%\")\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "from IPython.core.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "print(\"70%\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.probability import FreqDist\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print(\"100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#поскольку файл долго лемматизируется, сохраним его и загрузим уже готовый файл для дальнейшей работы с ним.\n",
    "def load(name, func):\n",
    "    exist = os.path.exists(name)\n",
    "    \n",
    "    print(f\"{'Reading' if exist else 'Executing'} {name}\")\n",
    "    \n",
    "    if name.endswith(\"csv\"):\n",
    "        if exist:\n",
    "            obj = pd.read_csv(name)\n",
    "            index = list(obj['Unnamed: 0'])\n",
    "            del obj['Unnamed: 0']\n",
    "            obj.index = index\n",
    "            return obj\n",
    "        else:\n",
    "            obj = func()\n",
    "            obj.to_csv(name)\n",
    "            return obj\n",
    "    elif name.endswith('json'):\n",
    "        if exist:\n",
    "            with open(name, 'rb') as f:\n",
    "                byt = f.read()\n",
    "            return json.loads(byt.decode('utf-8'))\n",
    "        else:\n",
    "            obj = func()\n",
    "            with open(name, 'wb') as f:\n",
    "                f.write(json.dumps(obj).encode('utf-8'))\n",
    "            return obj\n",
    "    else:\n",
    "        raise Exception('Undefined file type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['toxic', '0', '1'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#загрузка файла\n",
    "#на некоторых версиях on_bad_lines='warn'\n",
    "df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv',\n",
    "               sep=',', error_bad_lines=False, names=['sentence', 'label'])\n",
    "\n",
    "df['label'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence    Explanation\\nWhy the edits made under my usern...\n",
      "label                                                       0\n",
      "Name: 0.0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence label\n",
       "0.0  Explanation\\nWhy the edits made under my usern...     0\n",
       "1.0  D'aww! He matches this background colour I'm s...     0\n",
       "2.0  Hey man, I'm really not trying to edit war. It...     0\n",
       "3.0  \"\\nMore\\nI can't make any real suggestions on ...     0\n",
       "4.0  You, sir, are my hero. Any chance you remember...     0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#поскольку в данных выше можно увидеть, что среди уникалных значений есть еще значения помимо разметки о токсичности правок\n",
    "#присутствуют еще иные показатели, оставим только данные со значениями 0 и 1\n",
    "\n",
    "df = df[(df['label'] == '0') | (df['label'] == '1')]\n",
    "print(df.iloc[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# переведем разметку в тип int\n",
    "df['label'] = df['label'].apply(int)\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159292"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#таким образом, видно, что в данных остались только значения 0 и 1. Посмотрим длину датасета\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#напишем функцию для того, чтобы почистить текст, оставив только буквы из алфавита в нижнем регистре \n",
    "def clear_text(text):\n",
    "    re_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    split_text = re_text.split()\n",
    "    join_text = ' '.join(split_text)\n",
    "    return join_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#применим функцию к столбцу sentence\n",
    "df['sentence'] = df['sentence'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #напишем функцию для лемматизации данных \n",
    "# def lemm_one(row):\n",
    "#     return ' '.join([token.lemma_ for token in nlp(row)])\n",
    "# def lemmatize(text):\n",
    "#     return thread_map(lemm_one, text, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#файл загружен на мой юпитер\n",
    "#27:49\n",
    "df['lemm'] = zload('lemm.json', lambda: lemmatize(df['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выделим целевой признак\n",
    "X = df.drop(['label'], axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из данных, количество 0 сильно отличается от количества 1. Данные не сбалансированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим выборку на трейн и тест\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем TF-IDF. Выделим и удалим слова, которые не несут большой информативной нагрузки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим TfidfVectorizer к тестовой и трейновой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tf_idf = TfidfVectorizer(stop_words = stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tf_idf.fit_transform(X_train['lemm'])\n",
    "X_test_tfidf = tf_idf.transform(X_test['lemm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = X['sentence']\n",
    "y.value_counts()\n",
    "index_1 = y[y==0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Вывод:***  \n",
    "В данных можно увидеть, что среди уникалных значений есть были значения помимо разметки о токсичности правок, в ходе анализа удалили лишние данные.\n",
    "Провели лемматизацию и сделали TF-IDF. Выделили и удалили слова, которые не несли большой информативной нагрузки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "params = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tree = Pipeline([('tree', None)])  \n",
    "   \n",
    "pipeline_lg = Pipeline([('lg', None)])         \n",
    "\n",
    "pipeline_catboost = Pipeline([('cat', None)])                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    " \n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tree = [\n",
    "    {'tree': (RandomForestClassifier(random_state=1, class_weight = 'balanced'),),\n",
    "     'tree__n_estimators': range(1, 20, 10),\n",
    "     'tree__max_depth': range(1, 20, 10),\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "param_lg = [\n",
    "    {'lg': (LogisticRegression(random_state=1, class_weight = 'balanced'),),\n",
    "     }\n",
    "]\n",
    "\n",
    "param_catboost = [\n",
    "     {'cat': (CatBoostClassifier(random_state=1, verbose=False, class_weights = class_weights),),\n",
    "     'cat__learning_rate': np.logspace(-3, -1, 1),\n",
    "     'cat__iterations': range(25, 100, 100),\n",
    "     'cat__depth': range(5,10,5),\n",
    "      }\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = ['TREE', 'LG', 'CAT_BOOST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipelines.append(pipeline_tree)\n",
    "pipelines.append(pipeline_lg)\n",
    "pipelines.append(pipeline_catboost)\n",
    "\n",
    "\n",
    "params.append(param_tree)\n",
    "\n",
    "params.append(param_lg)\n",
    "\n",
    "params.append(param_catboost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "f1 = make_scorer(f1_score , average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TREE models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>rank_cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tree': RandomForestClassifier(class_weight='...</td>\n",
       "      <td>0.550635</td>\n",
       "      <td>1.856176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'tree': RandomForestClassifier(class_weight='...</td>\n",
       "      <td>0.505681</td>\n",
       "      <td>0.120870</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'tree': RandomForestClassifier(class_weight='...</td>\n",
       "      <td>0.286925</td>\n",
       "      <td>0.260259</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'tree': RandomForestClassifier(class_weight='...</td>\n",
       "      <td>0.238381</td>\n",
       "      <td>0.325428</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  cv_score  mean_fit_time  \\\n",
       "3  {'tree': RandomForestClassifier(class_weight='...  0.550635       1.856176   \n",
       "0  {'tree': RandomForestClassifier(class_weight='...  0.505681       0.120870   \n",
       "2  {'tree': RandomForestClassifier(class_weight='...  0.286925       0.260259   \n",
       "1  {'tree': RandomForestClassifier(class_weight='...  0.238381       0.325428   \n",
       "\n",
       "   rank_cv_score  \n",
       "3              1  \n",
       "0              2  \n",
       "2              3  \n",
       "1              4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:09,  9.22s/it]/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top LG models:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>rank_cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'lg': LogisticRegression(class_weight='balanc...</td>\n",
       "      <td>0.85935</td>\n",
       "      <td>40.22418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  cv_score  mean_fit_time  \\\n",
       "0  {'lg': LogisticRegression(class_weight='balanc...   0.85935       40.22418   \n",
       "\n",
       "   rank_cv_score  \n",
       "0              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:57, 102.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top CAT_BOOST models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>rank_cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'cat': &lt;catboost.core.CatBoostClassifier obje...</td>\n",
       "      <td>0.768955</td>\n",
       "      <td>55.309938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  cv_score  mean_fit_time  \\\n",
       "0  {'cat': <catboost.core.CatBoostClassifier obje...  0.768955      55.309938   \n",
       "\n",
       "   rank_cv_score  \n",
       "0              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [07:02, 140.70s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for pipeline, param_grid, name in tqdm(zip(pipelines, params, names)):   \n",
    "    gs = GridSearchCV(pipeline,\n",
    "                  param_grid,\n",
    "                  scoring=f1,\n",
    "                  n_jobs=-1,\n",
    "                  cv=3)\n",
    "\n",
    "    gs.fit(X_train_tfidf, y_train)  \n",
    "     \n",
    "     #top models table\n",
    "    print('Top ' + name + ' models:')\n",
    "    results_df = pd.DataFrame(gs.cv_results_)\n",
    "    results_df = results_df.sort_values(by=[\"rank_test_score\"])\n",
    "    display(results_df[['params', 'mean_test_score', 'mean_fit_time', 'rank_test_score']]\n",
    "            .head(5)\n",
    "            .rename(columns={'mean_test_score': 'cv_score'})\n",
    "            .rename(columns={'rank_test_score': 'rank_cv_score'}))\n",
    "\n",
    "    results[name] = gs     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Вывод:****   \n",
    "Как видно из данных, лучше всего себя показала модель LogisticRegression. Проведем проверку модели на тестовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [results[x].best_estimator_.predict(X_test_tfidf) for x in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TREE': 0.32510030090270814,\n",
       " 'LG': 0.7590705258866693,\n",
       " 'CAT_BOOST': 0.5665415102211097}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#выведем результат\n",
    "scores = {name:f1_score(y_test, pred) for name, pred in zip(names, preds)}\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****Вывод:*****  \n",
    "Как видно из данных, лучше всего себя показала модель LogisticRegression."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 381,
    "start_time": "2022-08-02T18:21:36.648Z"
   },
   {
    "duration": 2901,
    "start_time": "2022-08-02T18:21:37.321Z"
   },
   {
    "duration": 973,
    "start_time": "2022-08-02T18:21:48.879Z"
   },
   {
    "duration": 4403,
    "start_time": "2022-09-14T18:05:01.648Z"
   },
   {
    "duration": 4,
    "start_time": "2022-09-14T18:05:06.053Z"
   },
   {
    "duration": 2904,
    "start_time": "2022-09-14T18:05:06.059Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.967Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.968Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.969Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.971Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.972Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.974Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.975Z"
   },
   {
    "duration": 1,
    "start_time": "2022-09-14T18:05:08.976Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.978Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.980Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.981Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.983Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.984Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.986Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.987Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.988Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.989Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.991Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.992Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.994Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.996Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.997Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:08.999Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:05:09.000Z"
   },
   {
    "duration": 3012,
    "start_time": "2022-09-14T18:07:46.076Z"
   },
   {
    "duration": 3176,
    "start_time": "2022-09-14T18:09:13.498Z"
   },
   {
    "duration": 1985,
    "start_time": "2022-09-14T18:09:55.380Z"
   },
   {
    "duration": 41,
    "start_time": "2022-09-14T18:13:16.185Z"
   },
   {
    "duration": 7,
    "start_time": "2022-09-14T18:13:19.356Z"
   },
   {
    "duration": 15,
    "start_time": "2022-09-14T18:13:20.361Z"
   },
   {
    "duration": 2328,
    "start_time": "2022-09-14T18:17:19.217Z"
   },
   {
    "duration": 36,
    "start_time": "2022-09-14T18:17:21.547Z"
   },
   {
    "duration": 66,
    "start_time": "2022-09-14T18:17:21.585Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T18:17:21.653Z"
   },
   {
    "duration": 8,
    "start_time": "2022-09-14T18:17:21.657Z"
   },
   {
    "duration": 4094,
    "start_time": "2022-09-14T18:17:21.666Z"
   },
   {
    "duration": 534,
    "start_time": "2022-09-14T18:17:25.767Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T18:17:26.303Z"
   },
   {
    "duration": 114449,
    "start_time": "2022-09-14T18:17:26.308Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.759Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.760Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.761Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.762Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.763Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.764Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.764Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.765Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.766Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.767Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.768Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.769Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.770Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.771Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.772Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.772Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-14T18:19:20.773Z"
   },
   {
    "duration": 90533,
    "start_time": "2022-09-14T18:19:36.148Z"
   },
   {
    "duration": 136503,
    "start_time": "2022-09-14T18:22:22.250Z"
   },
   {
    "duration": 2,
    "start_time": "2022-09-14T18:25:04.803Z"
   },
   {
    "duration": 17081,
    "start_time": "2022-09-14T18:25:08.104Z"
   },
   {
    "duration": 25539,
    "start_time": "2022-09-14T18:27:15.186Z"
   },
   {
    "duration": 45462,
    "start_time": "2022-09-14T18:27:54.739Z"
   },
   {
    "duration": 550,
    "start_time": "2022-09-14T18:32:35.201Z"
   },
   {
    "duration": 58,
    "start_time": "2022-09-14T18:33:34.624Z"
   },
   {
    "duration": 6,
    "start_time": "2022-09-14T18:33:35.922Z"
   },
   {
    "duration": 38,
    "start_time": "2022-09-14T18:33:38.966Z"
   },
   {
    "duration": 1634,
    "start_time": "2022-09-14T18:33:40.791Z"
   },
   {
    "duration": 4,
    "start_time": "2022-09-14T18:33:49.010Z"
   },
   {
    "duration": 203,
    "start_time": "2022-09-14T18:33:50.088Z"
   },
   {
    "duration": 5,
    "start_time": "2022-09-14T18:33:56.664Z"
   },
   {
    "duration": 6913,
    "start_time": "2022-09-14T18:33:57.777Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T18:34:14.096Z"
   },
   {
    "duration": 4,
    "start_time": "2022-09-14T18:34:14.846Z"
   },
   {
    "duration": 15,
    "start_time": "2022-09-14T18:34:15.891Z"
   },
   {
    "duration": 4,
    "start_time": "2022-09-14T18:34:17.907Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T18:34:19.372Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T18:34:20.526Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T18:34:21.585Z"
   },
   {
    "duration": 422106,
    "start_time": "2022-09-14T18:34:22.887Z"
   },
   {
    "duration": 366,
    "start_time": "2022-09-14T18:41:24.995Z"
   },
   {
    "duration": 28,
    "start_time": "2022-09-14T18:41:25.363Z"
   },
   {
    "duration": 7,
    "start_time": "2022-09-14T18:44:06.361Z"
   },
   {
    "duration": 8,
    "start_time": "2022-09-14T19:03:52.567Z"
   },
   {
    "duration": 39,
    "start_time": "2022-09-14T19:03:54.202Z"
   },
   {
    "duration": 48,
    "start_time": "2022-09-14T19:03:55.036Z"
   },
   {
    "duration": 7,
    "start_time": "2022-09-14T19:08:30.502Z"
   },
   {
    "duration": 5,
    "start_time": "2022-09-14T19:08:40.634Z"
   },
   {
    "duration": 23,
    "start_time": "2022-09-14T19:08:41.434Z"
   },
   {
    "duration": 27,
    "start_time": "2022-09-14T19:08:42.748Z"
   },
   {
    "duration": 693,
    "start_time": "2022-09-14T19:15:43.537Z"
   },
   {
    "duration": 895,
    "start_time": "2022-09-14T19:15:47.941Z"
   },
   {
    "duration": 940,
    "start_time": "2022-09-14T19:15:58.661Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:16:20.897Z"
   },
   {
    "duration": 811,
    "start_time": "2022-09-14T19:16:23.459Z"
   },
   {
    "duration": 19,
    "start_time": "2022-09-14T19:16:25.734Z"
   },
   {
    "duration": 134,
    "start_time": "2022-09-14T19:16:27.713Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:16:29.351Z"
   },
   {
    "duration": 2,
    "start_time": "2022-09-14T19:16:31.749Z"
   },
   {
    "duration": 18,
    "start_time": "2022-09-14T19:16:34.345Z"
   },
   {
    "duration": 20,
    "start_time": "2022-09-14T19:18:29.998Z"
   },
   {
    "duration": 3032,
    "start_time": "2022-09-14T19:18:55.933Z"
   },
   {
    "duration": 1200,
    "start_time": "2022-09-14T19:19:02.937Z"
   },
   {
    "duration": 1107,
    "start_time": "2022-09-14T19:19:48.298Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:37:18.001Z"
   },
   {
    "duration": 2169,
    "start_time": "2022-09-14T19:37:21.042Z"
   },
   {
    "duration": 1028,
    "start_time": "2022-09-14T19:37:23.631Z"
   },
   {
    "duration": 29,
    "start_time": "2022-09-14T19:38:01.160Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:40:11.205Z"
   },
   {
    "duration": 1069,
    "start_time": "2022-09-14T19:40:17.005Z"
   },
   {
    "duration": 7,
    "start_time": "2022-09-14T19:41:26.299Z"
   },
   {
    "duration": 2,
    "start_time": "2022-09-14T19:41:26.943Z"
   },
   {
    "duration": 788,
    "start_time": "2022-09-14T19:41:27.699Z"
   },
   {
    "duration": 18,
    "start_time": "2022-09-14T19:41:29.298Z"
   },
   {
    "duration": 110,
    "start_time": "2022-09-14T19:41:30.271Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:41:32.114Z"
   },
   {
    "duration": 2,
    "start_time": "2022-09-14T19:41:33.552Z"
   },
   {
    "duration": 2157,
    "start_time": "2022-09-14T19:41:35.068Z"
   },
   {
    "duration": 1094,
    "start_time": "2022-09-14T19:41:38.444Z"
   },
   {
    "duration": 7,
    "start_time": "2022-09-14T19:44:52.228Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:44:54.902Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:44:55.751Z"
   },
   {
    "duration": 2278,
    "start_time": "2022-09-14T19:44:56.934Z"
   },
   {
    "duration": 1120,
    "start_time": "2022-09-14T19:44:59.214Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-14T19:45:18.742Z"
   },
   {
    "duration": 1048,
    "start_time": "2022-09-14T19:45:21.098Z"
   },
   {
    "duration": 921,
    "start_time": "2022-09-14T19:45:40.266Z"
   },
   {
    "duration": 4687,
    "start_time": "2022-09-15T19:15:17.759Z"
   },
   {
    "duration": 6,
    "start_time": "2022-09-15T19:15:39.285Z"
   },
   {
    "duration": 2952,
    "start_time": "2022-09-15T19:15:42.721Z"
   },
   {
    "duration": 34,
    "start_time": "2022-09-15T19:29:53.963Z"
   },
   {
    "duration": 52,
    "start_time": "2022-09-15T19:29:55.575Z"
   },
   {
    "duration": 5,
    "start_time": "2022-09-15T19:29:56.105Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-15T19:29:56.388Z"
   },
   {
    "duration": 3678,
    "start_time": "2022-09-15T19:29:56.813Z"
   },
   {
    "duration": 879,
    "start_time": "2022-09-15T19:30:01.773Z"
   },
   {
    "duration": 7,
    "start_time": "2022-09-15T19:30:37.317Z"
   },
   {
    "duration": 33,
    "start_time": "2022-09-15T19:30:41.509Z"
   },
   {
    "duration": 6,
    "start_time": "2022-09-15T19:30:41.838Z"
   },
   {
    "duration": 31,
    "start_time": "2022-09-15T19:30:44.089Z"
   },
   {
    "duration": 2,
    "start_time": "2022-09-15T19:30:44.758Z"
   },
   {
    "duration": 177,
    "start_time": "2022-09-15T19:30:45.995Z"
   },
   {
    "duration": 4,
    "start_time": "2022-09-15T19:30:47.488Z"
   },
   {
    "duration": 6092,
    "start_time": "2022-09-15T19:30:47.824Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
